# Prometheus Alerting Rules for golang-api-hexagonal
# This file defines alerting rules for the service
# Location: deploy/prometheus/alerts.yaml
#
# Usage: Load this file in Prometheus by adding to prometheus.yml:
#   rule_files:
#     - "alerts.yaml"
#
# Severity levels:
#   - critical: Immediate action required (pager-worthy)
#   - warning: Investigate soon (dashboard/ticket)

groups:
  - name: golang-api-hexagonal
    rules:
      # =============================================================================
      # HTTP Service Alerts
      # =============================================================================

      # HighErrorRate (Warning) - 5xx errors > 5% for 5 minutes
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: warning
          service: golang-api-hexagonal
        annotations:
          summary: "High HTTP error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} on {{ $labels.instance }}. More than 5% of requests are failing with 5xx status codes."
          runbook_url: "docs/runbook/high-error-rate.md"

      # HighErrorRateCritical - 5xx errors > 10% for 2 minutes
      - alert: HighErrorRateCritical
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total[5m]))
          ) > 0.10
        for: 2m
        labels:
          severity: critical
          service: golang-api-hexagonal
        annotations:
          summary: "Critical HTTP error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} on {{ $labels.instance }}. More than 10% of requests are failing!"
          runbook_url: "docs/runbook/high-error-rate.md"

      # HighLatency (Warning) - p95 latency > 500ms for 5 minutes
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
          > 0.5
        for: 5m
        labels:
          severity: warning
          service: golang-api-hexagonal
        annotations:
          summary: "High HTTP latency detected"
          description: "p95 latency is {{ $value | humanizeDuration }} on {{ $labels.instance }}. Threshold is 500ms."
          runbook_url: "docs/runbook/high-latency.md"

      # HighLatencyCritical - p95 latency > 1s for 2 minutes
      - alert: HighLatencyCritical
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))
          > 1.0
        for: 2m
        labels:
          severity: critical
          service: golang-api-hexagonal
        annotations:
          summary: "Critical HTTP latency detected"
          description: "p95 latency is {{ $value | humanizeDuration }} on {{ $labels.instance }}. Threshold is 1 second."
          runbook_url: "docs/runbook/high-latency.md"

      # ServiceDown - Service not responding for 1 minute
      - alert: ServiceDown
        expr: up{job="golang-api"} == 0
        for: 1m
        labels:
          severity: critical
          service: golang-api-hexagonal
        annotations:
          summary: "Service is down"
          description: "golang-api service on {{ $labels.instance }} has been down for more than 1 minute."
          runbook_url: "docs/runbook/service-down.md"

      # =============================================================================
      # Database Alerts
      # =============================================================================

      # DBConnectionExhausted - Database connection pool usage > 80%
      # Note: This alert requires custom db_pool metrics to be implemented.
      # Currently monitors via readiness endpoint. Replace expr when db_pool_usage_percent is available.
      - alert: DBConnectionExhausted
        expr: |
          (
            sum(rate(http_requests_total{path="/readyz", status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total{path="/readyz"}[5m]))
          ) > 0.20
        for: 5m
        labels:
          severity: warning
          service: golang-api-hexagonal
          component: database
        annotations:
          summary: "Database connectivity issues detected"
          description: "Readiness endpoint is failing at {{ $value | humanizePercentage }} rate. Check database connection pool and health."
          runbook_url: "docs/runbook/db-connection-exhausted.md"

      # DBSlowQueries - High percentage of slow database queries
      # Note: This alert uses HTTP latency as proxy for DB latency.
      # Replace with db_query_duration_seconds when available.
      - alert: DBSlowQueries
        expr: |
          histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{path=~"/api/v1/.*"}[5m])) by (le, path))
          > 0.5
        for: 5m
        labels:
          severity: warning
          service: golang-api-hexagonal
          component: database
        annotations:
          summary: "Slow database queries detected"
          description: "p95 latency for {{ $labels.path }} is {{ $value | humanizeDuration }}. May indicate slow database queries."
          runbook_url: "docs/runbook/db-slow-queries.md"

      # =============================================================================
      # Job Queue Alerts (Asynq)
      # =============================================================================

      # JobQueueBacklog - Pending jobs exceeds threshold
      # Note: Requires asynq_exporter for asynq_queue_size metric.
      # Using job processing rate as fallback indicator.
      - alert: JobQueueBacklog
        expr: |
          (
            sum(rate(job_processed_total{status="success"}[5m]))
            /
            sum(rate(job_processed_total[5m]))
          ) < 0.90
        for: 10m
        labels:
          severity: warning
          service: golang-api-hexagonal
          component: worker
        annotations:
          summary: "Job queue backlog detected"
          description: "Job success rate has dropped to {{ $value | humanizePercentage }}. This may indicate a backlog or processing issues."
          runbook_url: "docs/runbook/job-queue-backlog.md"

      # JobFailureRate - Job failure rate > 10%
      - alert: JobFailureRate
        expr: |
          (
            sum(rate(job_processed_total{status="failed"}[5m]))
            /
            sum(rate(job_processed_total[5m]))
          ) > 0.10
        for: 5m
        labels:
          severity: warning
          service: golang-api-hexagonal
          component: worker
        annotations:
          summary: "High job failure rate detected"
          description: "Job failure rate is {{ $value | humanizePercentage }} over the last 5 minutes. Check worker logs for errors."
          runbook_url: "docs/runbook/job-failure-rate.md"

      # JobFailureRateCritical - Job failure rate > 25%
      - alert: JobFailureRateCritical
        expr: |
          (
            sum(rate(job_processed_total{status="failed"}[5m]))
            /
            sum(rate(job_processed_total[5m]))
          ) > 0.25
        for: 2m
        labels:
          severity: critical
          service: golang-api-hexagonal
          component: worker
        annotations:
          summary: "Critical job failure rate detected"
          description: "Job failure rate is {{ $value | humanizePercentage }}. More than 25% of jobs are failing!"
          runbook_url: "docs/runbook/job-failure-rate.md"

      # JobProcessingStalled - No jobs processed for extended period
      - alert: JobProcessingStalled
        expr: |
          sum(rate(job_processed_total[5m])) == 0
          and
          sum(increase(job_processed_total[1h])) > 0
        for: 10m
        labels:
          severity: warning
          service: golang-api-hexagonal
          component: worker
        annotations:
          summary: "Job processing appears stalled"
          description: "No jobs have been processed in the last 10 minutes, but jobs were processed in the last hour. Worker may be stuck."
          runbook_url: "docs/runbook/job-queue-backlog.md"
